{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqR2NHlJP-qa",
        "outputId": "25eee8d9-c298-4e94-a0ba-dafb3d27744d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.7.0.72)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Installing collected packages: albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed albumentations-1.3.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install opendatasets\n",
        "!pip install -U albumentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/mlops/hw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwA0taq1QE22",
        "outputId": "671c878b-2194-4da5-ab9b-27be1b2b5148"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/mlops/hw2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.core.common import flatten\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "## import library\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from pycocotools.coco import COCO\n",
        "import urllib\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "#import opendatasets as od\n",
        "import pandas"
      ],
      "metadata": {
        "id": "eDRBxGFuQMlb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_path = ['/content/drive/MyDrive/mlops/hw2/chest-xray-pneumonia/chest_xray/train/',\n",
        "                '/content/drive/MyDrive/mlops/hw2/chest-xray-pneumonia/chest_xray/test/',\n",
        "                '/content/drive/MyDrive/mlops/hw2/chest-xray-pneumonia/chest_xray/val/']\n",
        "\n",
        "file_list_ = []\n",
        "for img_path in imgs_path:\n",
        " \n",
        "  file_list = glob.glob(img_path + \"*\")\n",
        "  for i in file_list: \n",
        "    file_list_.append(i)\n",
        "\n",
        "data = []\n",
        "\n",
        "for class_path in file_list_:\n",
        "            class_name = class_path.split(\"/\")[-1]\n",
        "            filenames = [f for f in os.listdir(class_path) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
        "            for img_path in filenames:\n",
        "                data.append([class_path+\"/\"+img_path, class_name])\n",
        "print(\"number of all data:\",len(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGeRfH41QVoP",
        "outputId": "a09313bc-e6e6-4f91-e22d-bd9bc482ccb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of all data: 5856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into 80% train and 20% test\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "val, test = train_test_split(test, test_size=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVy455anQVxW",
        "outputId": "688d2da5-a1eb-43f2-d66d-ea599962d805"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# train : 4684\n",
            "# val : 586\n",
            "# test : 586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,data, transform):\n",
        "        self.data = data\n",
        "        self.class_map = {\"NORMAL\" : 0, \"PNEUMONIA\": 1}\n",
        "        self.img_dim = (224, 224)\n",
        "\n",
        "    def rotateImage(image, angle):\n",
        "      row,col = (228,228)\n",
        "      center=tuple(np.array([row,col])/2)\n",
        "      rot_mat = cv2.getRotationMatrix2D(center,angle,1.0)\n",
        "      new_image = cv2.warpAffine(image, rot_mat, (col,row))\n",
        "      return new_image        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, self.img_dim)\n",
        "        transform = T.RandomHorizontalFlip()\n",
        "        if transform is True:\n",
        "          self.img = rotateImage(img ,random.randint(0, 30))\n",
        "          self.img = cv2.normalize(img, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "          img = transform(img)\n",
        "        \n",
        "        else:\n",
        "          self.img = cv2.normalize(img, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "        \n",
        "        img_tensor = torch.from_numpy(img)\n",
        "        class_id = self.class_map[class_name]\n",
        "        '''For this conversion we use the permute function of torch,\n",
        "            that allows us to change the ordering of the dimensions of a torch tensor. \n",
        "            The arguments we pass to it, correspond to the new ordering of dimensions we want.\n",
        "            For example in our case, we have (Width, Height, Channels).\n",
        "            (Width -> 0), (Height->1), (Channels->2)\n",
        "              We want to reorder these dimensions to make channels first, therefore,\n",
        "              we use img_tensor.permute(2, 0, 1), which would make the 2nd dimension first.'''\n",
        "        img_tensor = img_tensor.permute(2, 0, 1)\n",
        "        class_id = torch.tensor([class_id])\n",
        "        return img_tensor, class_id\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "r-9s_l0VQV1H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "dataset = CustomDataset(train,transform = True )\t\t\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset = CustomDataset(val ,transform = False)\t\t\n",
        "val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset = CustomDataset(test,transform = False)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "PzKO_AtvQV5g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQZM5IbHQV9s",
        "outputId": "f1623dad-10f5-4dd9-e06a-bce354b401d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "\n",
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'   \n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        # a flat distribution between 0 and 0.2\n",
        "        'distribution': 'uniform',\n",
        "        'min': 0,\n",
        "        'max': 0.2\n",
        "      },\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"MLops-HW2_1\")"
      ],
      "metadata": {
        "id": "VRIZhf2eQWHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from sklearn.metrics import accuracy_score\n",
        "def build_network():\n",
        "    # Load a pre-trained \n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Freeze all layers in the pre-trained \n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 1)\n",
        "    \n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    return model.to(device),  criterion.to(device)\n",
        "        \n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.fc.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.fc.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(model, criterion, train_loader,val_loader, optimizer):\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for inputs, labels in train_loader:\n",
        "        x = nn.Sigmoid()\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())\n",
        "        outputs = x(outputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"batch loss\": loss.item()})\n",
        "\n",
        "        #epoch_loss = running_loss / len(train_loader)\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print('epoch_loss: ', epoch_loss)\n",
        "\n",
        "    model.eval() \n",
        "\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs.float())\n",
        "        outputs = x(outputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        pred =torch.round(outputs.detach())\n",
        "        target = torch.round(labels.detach())\n",
        "        \n",
        "\n",
        "        y_pred.extend(pred.tolist())\n",
        "        y_true.extend(target.tolist())\n",
        "\n",
        "    val_acc = accuracy_score(y_true,y_pred)  \n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    print(f'Train Loss: {epoch_loss:.4f}\\t Val Loss: {val_loss:.4f}\\t Val Acc: {val_acc:.4f}')\n",
        "    print(f'val Loss: {epoch_loss:.4f}\\t Val Loss: {val_loss:.4f}\\t Val Acc: {val_loss:.4f}')\n",
        "    print(f'val acc: {epoch_loss:.4f}\\t Val Loss: {val_loss:.4f}\\t Val Acc: {val_acc:.4f}')\n",
        "    \n",
        "    return epoch_loss ,val_loss ,val_acc"
      ],
      "metadata": {
        "id": "61MhcZiJQt3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "def train(train_loader,val_loader,config=None):\n",
        "    \n",
        "    with wandb.init(config=config):\n",
        "        \n",
        "        config = wandb.config\n",
        "        \n",
        "        train_loader = train_loader\n",
        "        val_loader = val_loader\n",
        "        network, criterion = build_network()\n",
        "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "        num_epochs = 15\n",
        "        for epoch in range(num_epochs):\n",
        "            avg_loss,val_loss ,val_acc = train_epoch(network, criterion, train_loader,val_loader, optimizer)\n",
        "            wandb.log({\"loss_train\": avg_loss, \"epoch\": epoch})     \n",
        "            wandb.log({\"loss_val\": val_loss, \"epoch\": epoch})    \n",
        "            wandb.log({\"acc_val\": val_acc, \"epoch\": epoch})          "
      ],
      "metadata": {
        "id": "g_5S5EJzQuBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train(train_loader,val_loader), count=5)"
      ],
      "metadata": {
        "id": "nOkN_GTpQ1Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5kzQZkxls2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}